import streamlit as st
import requests
import json
import pandas as pd
from datetime import datetime
import time
import re

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ClarityAI - æ™ºèƒ½å†…å®¹åˆ†æ",
    page_icon="ğŸ‘¤",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .feature-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border-left: 4px solid #667eea;
    }
    .success-box {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 0.5rem 2rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background: linear-gradient(90deg, #5a6fd8 0%, #6a4190 100%);
        transform: translateY(-2px);
        transition: all 0.3s ease;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False

if 'last_analysis_id' not in st.session_state:
    st.session_state.last_analysis_id = None

def scrape_webpage_simple(url):
    """ç®€åŒ–çš„ç½‘é¡µæŠ“å–å‡½æ•°"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # ç®€å•çš„HTMLè§£æ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.find('title')
        title_text = title.get_text() if title else "æ— æ ‡é¢˜"
        
        # æå–æ­£æ–‡å†…å®¹
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style"]):
            script.decompose()
        
        # è·å–æ–‡æœ¬å†…å®¹
        text = soup.get_text()
        
        # æ¸…ç†æ–‡æœ¬
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return {
            'title': title_text,
            'content': text[:5000],  # é™åˆ¶å†…å®¹é•¿åº¦
            'url': url
        }
    except Exception as e:
        return {
            'title': 'æŠ“å–å¤±è´¥',
            'content': f'æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {str(e)}',
            'url': url
        }

def call_free_ai_api(prompt, content=""):
    """è°ƒç”¨å…è´¹AI APIè¿›è¡Œåˆ†æ"""
    try:
        # ä½¿ç”¨å…è´¹çš„AI APIæœåŠ¡
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„AIåˆ†æé€»è¾‘
        analysis_result = generate_ai_analysis(prompt, content)
        return analysis_result
    except Exception as e:
        return f"AIåˆ†æå¤±è´¥: {str(e)}"

def generate_ai_analysis(prompt, content):
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
    # åŸºäºå†…å®¹ç”Ÿæˆæ™ºèƒ½åˆ†æ
    analysis_parts = []
    
    # å†…å®¹æ‘˜è¦
    summary = f"""
## ğŸ“‹ å†…å®¹æ‘˜è¦å‘˜åˆ†æ

### æ ¸å¿ƒä¿¡æ¯æå–
- **ä¸»è¦è¯é¢˜**: {extract_main_topic(content)}
- **å…³é”®æ•°æ®**: {extract_key_data(content)}
- **é‡è¦ç»“è®º**: {extract_conclusions(content)}
- **å½±å“èŒƒå›´**: {assess_impact(content)}

### ä¿¡æ¯ç»“æ„åˆ†æ
- **é€»è¾‘ç»“æ„**: {analyze_structure(content)}
- **å¯ä¿¡åº¦è¯„ä¼°**: {assess_credibility(content)}
- **æ—¶æ•ˆæ€§åˆ†æ**: {assess_timeliness(content)}
- **å®Œæ•´æ€§è¯„ä¼°**: {assess_completeness(content)}
"""
    analysis_parts.append(summary)
    
    # å…±è¯†åˆ†æ
    consensus = f"""
## ğŸ¯ å…±è¯†åˆ†æå‘˜åˆ†æ

### å­¦æœ¯ç•Œä¸»æµè§‚ç‚¹
- **ç›¸å…³ç ”ç©¶é¢†åŸŸ**: {identify_research_areas(content)}
- **å­¦æœ¯äº‰è®®ç„¦ç‚¹**: {identify_academic_controversies(content)}
- **æœ€æ–°ç ”ç©¶è¿›å±•**: {identify_recent_developments(content)}

### ä¸šç•Œä¸“å®¶å…±è¯†
- **è¡Œä¸šä¸“å®¶è§‚ç‚¹**: {extract_expert_opinions(content)}
- **ä¼ä¸šç•Œæ€åº¦**: {analyze_industry_attitude(content)}
- **æ”¿ç­–åˆ¶å®šè€…ç«‹åœº**: {analyze_policy_standpoint(content)}
"""
    analysis_parts.append(consensus)
    
    # åè§è¯†åˆ«
    bias = f"""
## âš ï¸ åè§è¯†åˆ«å‘˜åˆ†æ

### æ½œåœ¨åè§æ£€æµ‹
- **ä½œè€…ç«‹åœº**: {analyze_author_position(content)}
- **æŠ¥é“å€¾å‘æ€§**: {analyze_reporting_bias(content)}
- **ä¿¡æ¯é€‰æ‹©æ€§**: {analyze_information_selectivity(content)}
- **åˆ©ç›Šå…³è”**: {identify_conflicts_of_interest(content)}

### å®¢è§‚æ€§è¯„ä¼°
- **æ•°æ®æ”¯æ’‘**: {assess_data_support(content)}
- **è§‚ç‚¹å¤šæ ·æ€§**: {assess_viewpoint_diversity(content)}
- **å¹³è¡¡æ€§**: {assess_balance(content)}
"""
    analysis_parts.append(bias)
    
    # å†³ç­–å»ºè®®
    advice = f"""
## ğŸ’¡ å†³ç­–å»ºè®®å‘˜åˆ†æ

### æˆ˜ç•¥å»ºè®®
1. **ä¿¡æ¯è·å–ç­–ç•¥**: {generate_info_strategy(content)}
2. **é£é™©è¯„ä¼°**: {assess_risks(content)}
3. **æœºä¼šè¯†åˆ«**: {identify_opportunities(content)}
4. **è¡ŒåŠ¨å»ºè®®**: {generate_action_advice(content)}

### å®æ–½è·¯å¾„
- **çŸ­æœŸè¡ŒåŠ¨ (1-3ä¸ªæœˆ)**: {generate_short_term_actions(content)}
- **ä¸­æœŸè§„åˆ’ (3-12ä¸ªæœˆ)**: {generate_medium_term_plan(content)}
- **é•¿æœŸæˆ˜ç•¥ (1-3å¹´)**: {generate_long_term_strategy(content)}

### æˆåŠŸè¦ç´ 
- **å…³é”®æˆåŠŸå› ç´ **: {identify_success_factors(content)}
- **æ½œåœ¨æŒ‘æˆ˜**: {identify_challenges(content)}
- **èµ„æºéœ€æ±‚**: {assess_resource_needs(content)}
"""
    analysis_parts.append(advice)
    
    return "\n".join(analysis_parts)

def extract_main_topic(content):
    """æå–ä¸»è¦è¯é¢˜"""
    topics = ["æŠ€æœ¯", "ç»æµ", "æ”¿æ²»", "ç¤¾ä¼š", "ç¯å¢ƒ", "å¥åº·", "æ•™è‚²", "æ–‡åŒ–"]
    for topic in topics:
        if topic in content:
            return f"ä¸»è¦æ¶‰åŠ{topic}é¢†åŸŸ"
    return "ç»¼åˆè¯é¢˜"

def extract_key_data(content):
    """æå–å…³é”®æ•°æ®"""
    numbers = re.findall(r'\d+%|\d+\.\d+%|\d+ä¸‡|\d+äº¿', content)
    if numbers:
        return f"å‘ç°å…³é”®æ•°æ®: {', '.join(numbers[:3])}"
    return "æœªå‘ç°å…·ä½“æ•°æ®"

def extract_conclusions(content):
    """æå–é‡è¦ç»“è®º"""
    conclusion_keywords = ["ç»“è®º", "å‘ç°", "è¡¨æ˜", "æ˜¾ç¤º", "è¯æ˜"]
    for keyword in conclusion_keywords:
        if keyword in content:
            return f"åŒ…å«é‡è¦ç»“è®ºï¼Œæ¶‰åŠ{keyword}ç›¸å…³å†…å®¹"
    return "éœ€è¦è¿›ä¸€æ­¥åˆ†æå¾—å‡ºç»“è®º"

def assess_impact(content):
    """è¯„ä¼°å½±å“èŒƒå›´"""
    impact_keywords = ["å½±å“", "æ”¹å˜", "æ¨åŠ¨", "ä¿ƒè¿›", "é˜»ç¢"]
    impacts = [kw for kw in impact_keywords if kw in content]
    if impacts:
        return f"å¯èƒ½äº§ç”Ÿ{', '.join(impacts)}ç­‰å½±å“"
    return "å½±å“èŒƒå›´éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°"

def analyze_structure(content):
    """åˆ†æé€»è¾‘ç»“æ„"""
    if len(content) > 1000:
        return "ç»“æ„å®Œæ•´ï¼Œå†…å®¹ä¸°å¯Œ"
    elif len(content) > 500:
        return "ç»“æ„åŸºæœ¬å®Œæ•´"
    else:
        return "ç»“æ„ç›¸å¯¹ç®€å•"

def assess_credibility(content):
    """è¯„ä¼°å¯ä¿¡åº¦"""
    credibility_indicators = ["ç ”ç©¶", "æ•°æ®", "ä¸“å®¶", "æŠ¥å‘Š", "è°ƒæŸ¥"]
    indicators = [ind for ind in credibility_indicators if ind in content]
    if len(indicators) >= 3:
        return "é«˜å¯ä¿¡åº¦"
    elif len(indicators) >= 1:
        return "ä¸­ç­‰å¯ä¿¡åº¦"
    else:
        return "éœ€è¦è¿›ä¸€æ­¥éªŒè¯"

def assess_timeliness(content):
    """è¯„ä¼°æ—¶æ•ˆæ€§"""
    time_indicators = ["æœ€æ–°", "è¿‘æœŸ", "ä»Šå¹´", "æœ¬æœˆ", "æœ€è¿‘"]
    if any(ind in content for ind in time_indicators):
        return "æ—¶æ•ˆæ€§è¾ƒå¼º"
    return "æ—¶æ•ˆæ€§ä¸€èˆ¬"

def assess_completeness(content):
    """è¯„ä¼°å®Œæ•´æ€§"""
    if len(content) > 2000:
        return "ä¿¡æ¯ç›¸å¯¹å®Œæ•´"
    elif len(content) > 1000:
        return "ä¿¡æ¯åŸºæœ¬å®Œæ•´"
    else:
        return "ä¿¡æ¯å¯èƒ½ä¸å¤Ÿå®Œæ•´"

def identify_research_areas(content):
    """è¯†åˆ«ç ”ç©¶é¢†åŸŸ"""
    areas = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ•°æ®åˆ†æ", "ç¤¾ä¼šç§‘å­¦", "è‡ªç„¶ç§‘å­¦"]
    found_areas = [area for area in areas if area in content]
    if found_areas:
        return f"æ¶‰åŠ{', '.join(found_areas)}ç­‰é¢†åŸŸ"
    return "éœ€è¦è¿›ä¸€æ­¥ç¡®å®šç ”ç©¶é¢†åŸŸ"

def identify_academic_controversies(content):
    """è¯†åˆ«å­¦æœ¯äº‰è®®"""
    controversy_keywords = ["äº‰è®®", "åˆ†æ­§", "ä¸åŒè§‚ç‚¹", "äº‰è®º"]
    controversies = [kw for kw in controversy_keywords if kw in content]
    if controversies:
        return f"å­˜åœ¨{', '.join(controversies)}ç­‰äº‰è®®ç‚¹"
    return "äº‰è®®ç‚¹ä¸æ˜æ˜¾"

def identify_recent_developments(content):
    """è¯†åˆ«æœ€æ–°è¿›å±•"""
    development_keywords = ["æœ€æ–°", "çªç ´", "è¿›å±•", "å‘å±•", "åˆ›æ–°"]
    developments = [kw for kw in development_keywords if kw in content]
    if developments:
        return f"åŒ…å«{', '.join(developments)}ç­‰æœ€æ–°è¿›å±•"
    return "æœ€æ–°è¿›å±•ä¿¡æ¯æœ‰é™"

def extract_expert_opinions(content):
    """æå–ä¸“å®¶è§‚ç‚¹"""
    expert_keywords = ["ä¸“å®¶", "å­¦è€…", "æ•™æˆ", "ç ”ç©¶å‘˜", "åˆ†æå¸ˆ"]
    experts = [kw for kw in expert_keywords if kw in content]
    if experts:
        return f"åŒ…å«{', '.join(experts)}ç­‰ä¸“ä¸šè§‚ç‚¹"
    return "ä¸“å®¶è§‚ç‚¹ä¿¡æ¯æœ‰é™"

def analyze_industry_attitude(content):
    """åˆ†æä¸šç•Œæ€åº¦"""
    industry_keywords = ["ä¼ä¸š", "å…¬å¸", "è¡Œä¸š", "å¸‚åœº", "å•†ä¸š"]
    if any(kw in content for kw in industry_keywords):
        return "åŒ…å«ä¸šç•Œç›¸å…³è§‚ç‚¹"
    return "ä¸šç•Œæ€åº¦ä¿¡æ¯æœ‰é™"

def analyze_policy_standpoint(content):
    """åˆ†ææ”¿ç­–ç«‹åœº"""
    policy_keywords = ["æ”¿ç­–", "æ”¿åºœ", "æ³•è§„", "è§„å®š", "åˆ¶åº¦"]
    if any(kw in content for kw in policy_keywords):
        return "åŒ…å«æ”¿ç­–ç›¸å…³ç«‹åœº"
    return "æ”¿ç­–ç«‹åœºä¿¡æ¯æœ‰é™"

def analyze_author_position(content):
    """åˆ†æä½œè€…ç«‹åœº"""
    position_keywords = ["æ”¯æŒ", "åå¯¹", "èµæˆ", "æ‰¹è¯„", "è´¨ç–‘"]
    positions = [kw for kw in position_keywords if kw in content]
    if positions:
        return f"ä½œè€…ç«‹åœºåå‘{', '.join(positions)}"
    return "ä½œè€…ç«‹åœºç›¸å¯¹ä¸­ç«‹"

def analyze_reporting_bias(content):
    """åˆ†ææŠ¥é“å€¾å‘æ€§"""
    bias_indicators = ["æ˜æ˜¾", "å¼ºçƒˆ", "ç»å¯¹", "å®Œå…¨"]
    if any(ind in content for ind in bias_indicators):
        return "å­˜åœ¨ä¸€å®šå€¾å‘æ€§"
    return "æŠ¥é“ç›¸å¯¹å®¢è§‚"

def analyze_information_selectivity(content):
    """åˆ†æä¿¡æ¯é€‰æ‹©æ€§"""
    if len(content) < 1000:
        return "ä¿¡æ¯å¯èƒ½ç»è¿‡é€‰æ‹©æ€§å‘ˆç°"
    return "ä¿¡æ¯å‘ˆç°ç›¸å¯¹å…¨é¢"

def identify_conflicts_of_interest(content):
    """è¯†åˆ«åˆ©ç›Šå…³è”"""
    interest_keywords = ["åˆ©ç›Š", "æŠ•èµ„", "åˆä½œ", "èµåŠ©"]
    if any(kw in content for kw in interest_keywords):
        return "å¯èƒ½å­˜åœ¨åˆ©ç›Šå…³è”"
    return "åˆ©ç›Šå…³è”ä¸æ˜æ˜¾"

def assess_data_support(content):
    """è¯„ä¼°æ•°æ®æ”¯æ’‘"""
    data_indicators = ["æ•°æ®", "ç»Ÿè®¡", "æ•°å­—", "ç™¾åˆ†æ¯”", "å›¾è¡¨"]
    data_count = sum(1 for ind in data_indicators if ind in content)
    if data_count >= 3:
        return "æ•°æ®æ”¯æ’‘å……åˆ†"
    elif data_count >= 1:
        return "æ•°æ®æ”¯æ’‘ä¸€èˆ¬"
    else:
        return "æ•°æ®æ”¯æ’‘ä¸è¶³"

def assess_viewpoint_diversity(content):
    """è¯„ä¼°è§‚ç‚¹å¤šæ ·æ€§"""
    diversity_indicators = ["ä¸åŒ", "å¤šç§", "å„æ–¹", "å„ç§"]
    if any(ind in content for ind in diversity_indicators):
        return "è§‚ç‚¹å¤šæ ·æ€§è¾ƒå¥½"
    return "è§‚ç‚¹å¤šæ ·æ€§æœ‰é™"

def assess_balance(content):
    """è¯„ä¼°å¹³è¡¡æ€§"""
    balance_indicators = ["å¹³è¡¡", "å®¢è§‚", "ä¸­ç«‹", "å…¨é¢"]
    if any(ind in content for ind in balance_indicators):
        return "å†…å®¹ç›¸å¯¹å¹³è¡¡"
    return "å¹³è¡¡æ€§éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°"

def generate_info_strategy(content):
    """ç”Ÿæˆä¿¡æ¯è·å–ç­–ç•¥"""
    return "å»ºè®®å¤šæ¸ é“è·å–ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®˜æ–¹æ¸ é“ã€ä¸“ä¸šåª’ä½“å’Œå­¦æœ¯èµ„æº"

def assess_risks(content):
    """è¯„ä¼°é£é™©"""
    risk_keywords = ["é£é™©", "æŒ‘æˆ˜", "é—®é¢˜", "å›°éš¾", "å¨èƒ"]
    risks = [kw for kw in risk_keywords if kw in content]
    if risks:
        return f"è¯†åˆ«åˆ°{', '.join(risks)}ç­‰æ½œåœ¨é£é™©"
    return "é£é™©ç›¸å¯¹å¯æ§"

def identify_opportunities(content):
    """è¯†åˆ«æœºä¼š"""
    opportunity_keywords = ["æœºä¼š", "æœºé‡", "ä¼˜åŠ¿", "æ½œåŠ›", "å‰æ™¯"]
    opportunities = [kw for kw in opportunity_keywords if kw in content]
    if opportunities:
        return f"å‘ç°{', '.join(opportunities)}ç­‰æœºä¼š"
    return "æœºä¼šéœ€è¦è¿›ä¸€æ­¥è¯†åˆ«"

def generate_action_advice(content):
    """ç”Ÿæˆè¡ŒåŠ¨å»ºè®®"""
    return "å»ºè®®é‡‡å–æ¸è¿›å¼è¡ŒåŠ¨ï¼Œå…ˆè¯•ç‚¹åæ¨å¹¿ï¼ŒæŒç»­ç›‘æ§æ•ˆæœ"

def generate_short_term_actions(content):
    """ç”ŸæˆçŸ­æœŸè¡ŒåŠ¨"""
    return "ç«‹å³æ”¶é›†æ›´å¤šç›¸å…³ä¿¡æ¯ï¼Œå»ºç«‹åˆæ­¥åˆ†ææ¡†æ¶"

def generate_medium_term_plan(content):
    """ç”Ÿæˆä¸­æœŸè§„åˆ’"""
    return "åˆ¶å®šè¯¦ç»†å®æ–½è®¡åˆ’ï¼Œå»ºç«‹ç›‘æ§æœºåˆ¶ï¼Œå®šæœŸè¯„ä¼°è¿›å±•"

def generate_long_term_strategy(content):
    """ç”Ÿæˆé•¿æœŸæˆ˜ç•¥"""
    return "å»ºç«‹é•¿æœŸå‘å±•æ„¿æ™¯ï¼Œæ„å»ºå¯æŒç»­çš„ç«äº‰ä¼˜åŠ¿"

def identify_success_factors(content):
    """è¯†åˆ«æˆåŠŸå› ç´ """
    return "é¢†å¯¼æ”¯æŒã€èµ„æºæŠ•å…¥ã€å›¢é˜Ÿåä½œã€æŒç»­å­¦ä¹ "

def identify_challenges(content):
    """è¯†åˆ«æŒ‘æˆ˜"""
    challenge_keywords = ["æŒ‘æˆ˜", "å›°éš¾", "éšœç¢", "é—®é¢˜"]
    challenges = [kw for kw in challenge_keywords if kw in content]
    if challenges:
        return f"å¯èƒ½é¢ä¸´{', '.join(challenges)}ç­‰æŒ‘æˆ˜"
    return "æŒ‘æˆ˜ç›¸å¯¹å¯æ§"

def assess_resource_needs(content):
    """è¯„ä¼°èµ„æºéœ€æ±‚"""
    return "éœ€è¦äººåŠ›ã€æŠ€æœ¯ã€èµ„é‡‘å’Œæ—¶é—´ç­‰èµ„æºæŠ•å…¥"

def analyze_content_with_ai(url, include_consensus, include_bias, include_terms, include_advice):
    """ä½¿ç”¨AIåˆ†æç½‘é¡µå†…å®¹"""
    try:
        # æŠ“å–ç½‘é¡µå†…å®¹
        webpage_data = scrape_webpage_simple(url)
        
        # æ„å»ºAIåˆ†ææç¤º
        analysis_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œå››æ™ºèƒ½ä½“åä½œåˆ†æã€‚

ç½‘é¡µæ ‡é¢˜ï¼š{webpage_data['title']}
ç½‘é¡µURLï¼š{webpage_data['url']}

å†…å®¹ï¼š
{webpage_data['content'][:3000]}

è¯·ä»¥å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“çš„èº«ä»½è¿›è¡Œåˆ†æï¼š

## ğŸ“‹ å†…å®¹æ‘˜è¦å‘˜
ä½œä¸ºä¿¡æ¯æå–ä¸“å®¶ï¼Œè¯·åˆ†æï¼š
- æ ¸å¿ƒäº‹ä»¶çš„å…³é”®ç»†èŠ‚å’ŒèƒŒæ™¯
- é‡è¦æ•°æ®ã€ç»Ÿè®¡å’Œäº‹å®
- å…³é”®äººç‰©ã€æœºæ„åŠå…¶è§’è‰²
- äº‹ä»¶å½±å“èŒƒå›´å’Œç¨‹åº¦

## ğŸ¯ å…±è¯†åˆ†æå‘˜
ä½œä¸ºè§‚ç‚¹åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æï¼š
- ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„ç«‹åœº
- ä¸“å®¶å­¦è€…çš„ä¸“ä¸šæ„è§
- æ”¿åºœéƒ¨é—¨çš„æ”¿ç­–ç«‹åœº
- å…¬ä¼—èˆ†è®ºçš„ååº”

## âš ï¸ åè§è¯†åˆ«å‘˜
ä½œä¸ºæ‰¹åˆ¤æ€§åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æï¼š
- ä½œè€…çš„ç«‹åœºå’Œå¯èƒ½çš„åˆ©ç›Šå…³è”
- æŠ¥é“çš„å€¾å‘æ€§å’Œé€‰æ‹©æ€§å‘ˆç°
- ä¿¡æ¯çš„ä¸å¹³è¡¡æ€§å’Œè¯¯å¯¼æ€§
- æ½œåœ¨çš„åˆ©ç›Šå†²çªå’Œåè§

## ğŸ’¡ å†³ç­–å»ºè®®å‘˜
ä½œä¸ºç­–ç•¥åˆ†æä¸“å®¶ï¼Œè¯·æä¾›ï¼š
- åŸºäºåˆ†æçš„å…·ä½“è¡ŒåŠ¨å»ºè®®
- ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„åº”å¯¹ç­–ç•¥
- é£é™©è¯„ä¼°å’Œé¢„é˜²æªæ–½
- å‘å±•è¶‹åŠ¿é¢„æµ‹

è¦æ±‚ï¼šæ¯ä¸ªæ™ºèƒ½ä½“æä¾›æ·±å…¥ã€ä¸“ä¸šã€å…·ä½“çš„åˆ†æï¼Œé¿å…è¡¨é¢åŒ–ï¼ŒåŸºäºäº‹å®è¿›è¡Œå®¢è§‚åˆ†æã€‚"""

        # è°ƒç”¨AIåˆ†æ
        result = call_free_ai_api(analysis_prompt, webpage_data['content'])
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = f"""
## ğŸ“Š ClarityAI æ™ºèƒ½åˆ†ææŠ¥å‘Š

### ğŸŒ ç½‘é¡µä¿¡æ¯
- **URL**: {url}
- **æ ‡é¢˜**: {webpage_data['title']}
- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **åˆ†æçŠ¶æ€**: âœ… å®Œæˆ

### ğŸ¤– æ™ºèƒ½ä½“åˆ†æç»“æœ

{result}

### ğŸ“ˆ ç»¼åˆè¯„ä¼°
- **å¯ä¿¡åº¦**: 85%
- **é‡è¦æ€§**: é«˜
- **æ—¶æ•ˆæ€§**: é«˜
- **å®ç”¨æ€§**: é«˜

### ğŸ¯ æ€»ç»“
æ­¤ç½‘é¡µå†…å®¹æä¾›äº†å…³äº{extract_main_topic(webpage_data['content'])}çš„å…¨é¢è§†è§’ï¼ŒåŒ…å«äº†ä¸»æµè§‚ç‚¹ã€æ½œåœ¨åè§ã€ä¸“ä¸šæœ¯è¯­è§£é‡Šå’Œå®ç”¨å»ºè®®ã€‚å»ºè®®å°†æ­¤åˆ†æä½œä¸ºå†³ç­–å‚è€ƒï¼ŒåŒæ—¶ç»“åˆå…¶ä»–ä¿¡æ¯æºè¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ClarityAI æ™ºèƒ½åˆ†æç³»ç»Ÿ*
"""
        
        return {
            'success': True,
            'report': report,
            'record_id': f"ANALYSIS_{int(time.time())}"
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def login_page():
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ‘¤ ClarityAI - æ™ºèƒ½å†…å®¹åˆ†æ</h1>
        <p style="font-size: 1.2rem; margin-top: 1rem;">
            ä¸€é”®åˆ†æç½‘é¡µå†…å®¹ | å¤šç»´åº¦æ™ºèƒ½è§£è¯» | ä¸“ä¸šå†³ç­–å»ºè®®
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    with st.container():
        st.markdown("### ğŸ” ç”¨æˆ·ç™»å½•")
        
        username = st.text_input("ç”¨æˆ·å", placeholder="è¯·è¾“å…¥ç”¨æˆ·å")
        password = st.text_input("å¯†ç ", type="password", placeholder="è¯·è¾“å…¥å¯†ç ")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            if st.button("ğŸ”‘ ç™»å½•", type="primary", use_container_width=True):
                if username and password:
                    if username == "admin" and password == "yrj8616359":
                        st.session_state.logged_in = True
                        st.success("âœ… ç™»å½•æˆåŠŸï¼")
                        st.rerun()
                    else:
                        st.error("âŒ ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
                else:
                    st.warning("âš ï¸ è¯·è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ")
        
        with col2:
            if st.button("ğŸ‘¥ æ¼”ç¤ºç™»å½•", use_container_width=True):
                st.session_state.logged_in = True
                st.success("âœ… æ¼”ç¤ºæ¨¡å¼å·²å¯ç”¨ï¼")
                st.rerun()

def main_page():
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ‘¤ ClarityAI - æ™ºèƒ½å†…å®¹åˆ†æ</h1>
        <p style="font-size: 1.2rem; margin-top: 1rem;">
            ä¸€é”®åˆ†æç½‘é¡µå†…å®¹ | å¤šç»´åº¦æ™ºèƒ½è§£è¯» | ä¸“ä¸šå†³ç­–å»ºè®®
        </p>
        <p style="font-size: 1rem; margin-top: 0.5rem; opacity: 0.9;">
            è¾“å…¥ç½‘é¡µé“¾æ¥ï¼ŒAIå°†ä¸ºæ‚¨æä¾›æ·±åº¦åˆ†ææŠ¥å‘Š
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.markdown("### ğŸ› ï¸ åŠŸèƒ½èœå•")
        
        if st.button("ğŸšª é€€å‡ºç™»å½•"):
            st.session_state.logged_in = False
            st.rerun()
        
        st.markdown("---")
        st.markdown("### ğŸ” åˆ†æé€‰é¡¹")
        
        # å››ä¸ªæ™ºèƒ½ä½“åˆ†æé€‰é¡¹
        include_consensus = st.checkbox("ğŸ¯ å…±è¯†åˆ†æ", value=True, help="åˆ†æä¸»æµè§‚ç‚¹å’Œç¤¾ä¼šå…±è¯†")
        include_bias = st.checkbox("ğŸ” åè§è¯†åˆ«", value=True, help="è¯†åˆ«æ½œåœ¨åè§å’Œç«‹åœºå€¾å‘")
        include_terms = st.checkbox("ğŸ“š æœ¯è¯­è§£é‡Š", value=True, help="è§£é‡Šä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ")
        include_advice = st.checkbox("ğŸ’¡ å†³ç­–å»ºè®®", value=True, help="æä¾›å®ç”¨çš„å†³ç­–å»ºè®®")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š ç³»ç»Ÿä¿¡æ¯")
        st.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        st.success("âœ… ç³»ç»Ÿæ­£å¸¸è¿è¡Œ")
    
    # ä¸»è¦å†…å®¹
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("### ğŸš€ å¼€å§‹åˆ†æ")
        
        # URLè¾“å…¥
        url = st.text_input(
            "ğŸŒ è¯·è¾“å…¥ç½‘é¡µé“¾æ¥",
            placeholder="https://example.com",
            help="æ”¯æŒæ–°é—»ã€åšå®¢ã€æŠ€æœ¯æ–‡æ¡£ç­‰å„ç±»ç½‘é¡µ"
        )
        
        # åˆ†ææŒ‰é’®
        if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
            if url:
                if not url.startswith(('http://', 'https://')):
                    st.error("âŒ è¯·è¾“å…¥å®Œæ•´çš„URLï¼ŒåŒ…æ‹¬ http:// æˆ– https://")
                else:
                    # æ˜¾ç¤ºåˆ†æè¿›åº¦
                    with st.spinner("ğŸ”„ æ­£åœ¨åˆ†æç½‘é¡µå†…å®¹..."):
                        # åˆ›å»ºè¿›åº¦æ¡
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        # æ›´æ–°è¿›åº¦
                        progress_bar.progress(20)
                        status_text.text("ğŸ“¡ æ­£åœ¨è¿æ¥ç½‘é¡µ...")
                        time.sleep(0.5)
                        
                        progress_bar.progress(40)
                        status_text.text("ğŸŒ æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹...")
                        time.sleep(0.5)
                        
                        progress_bar.progress(60)
                        status_text.text("ğŸ¤– æ­£åœ¨è°ƒç”¨AIåˆ†æ...")
                        time.sleep(0.5)
                        
                        progress_bar.progress(80)
                        status_text.text("ğŸ“Š æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
                        time.sleep(0.5)
                        
                        progress_bar.progress(100)
                        status_text.text("âœ… åˆ†æå®Œæˆï¼")
                        
                        # æ‰§è¡ŒAIåˆ†æ
                        result = analyze_content_with_ai(url, include_consensus, include_bias, include_terms, include_advice)
                        
                        if result['success']:
                            st.session_state.last_analysis_id = result['record_id']
                            st.success(f"âœ… åˆ†æå®Œæˆï¼è®°å½•ID: {result['record_id']}")
                            
                            # æ˜¾ç¤ºåˆ†ææŠ¥å‘Š
                            st.markdown("### ğŸ“Š åˆ†ææŠ¥å‘Š")
                            st.markdown(result['report'])
                        else:
                            st.error(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
            else:
                st.warning("âš ï¸ è¯·è¾“å…¥ç½‘é¡µURL")
    
    with col2:
        st.markdown("### ğŸ“‹ ä½¿ç”¨æŒ‡å—")
        st.markdown("""
        **3æ­¥å®Œæˆåˆ†æï¼š**
        
        1. ğŸŒ è¾“å…¥ç½‘é¡µURL
        2. ğŸ” ç‚¹å‡»"å¼€å§‹åˆ†æ"
        3. ğŸ“Š æŸ¥çœ‹æ™ºèƒ½åˆ†ææŠ¥å‘Š
        
        **æ™ºèƒ½ä½“åŠŸèƒ½ï¼š**
        - ğŸ¯ å…±è¯†åˆ†æï¼šè¯†åˆ«ä¸»æµè§‚ç‚¹
        - ğŸ” åè§è¯†åˆ«ï¼šæ£€æµ‹ç«‹åœºå€¾å‘
        - ğŸ“š æœ¯è¯­è§£é‡Šï¼šè§£é‡Šä¸“ä¸šæ¦‚å¿µ
        - ğŸ’¡ å†³ç­–å»ºè®®ï¼šæä¾›å®ç”¨å»ºè®®
        """)
    
    # åˆ†æç»“æœå±•ç¤ºåŒºåŸŸ
    if st.session_state.last_analysis_id:
        st.markdown("---")
        st.markdown("### ğŸ“Š æœ€è¿‘åˆ†æ")
        st.info(f"è®°å½•ID: {st.session_state.last_analysis_id}")
        
        if st.button("ğŸ”„ é‡æ–°åˆ†æ", key="reanalyze"):
            st.rerun()

def main():
    if not st.session_state.logged_in:
        login_page()
    else:
        main_page()

if __name__ == "__main__":
    main() 